# LLMs Tool  
![Authour](https://img.shields.io/badge/Author-StanleyLsx-red.svg) 
[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
![python_version](https://img.shields.io/badge/Python-3.10%2B-green.svg)
[![torch_version](https://img.shields.io/badge/torch-2.0%2B-pink.svg)](requirements.txt)


## Introduction
ä¸€ä¸ªåŸºäºğŸ¤—[HuggingFace](https://huggingface.co/)å¼€å‘çš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒã€æµ‹è¯•å·¥å…·ã€‚æ”¯æŒä¸åŒæ¨¡å‹çš„webuiã€ç»ˆç«¯é¢„æµ‹ï¼Œæ”¯æŒå„æ¨¡å‹çš„ä½å‚æ•°é‡åŠå…¨å‚æ•°æ¨¡å‹è®­ç»ƒå’Œèåˆï¼ŒRLHFè®­ç»ƒ(PPOå’ŒDPOä¸¤ç§æ–¹æ³•)ã€‚åŒæ—¶æ”¯æŒdeepspeedè®­ç»ƒã€‚  

ä½œè€…ä¹ æƒ¯äºæŠŠé…ç½®å’Œè¦åšçš„äº‹æƒ…éƒ½å†™åœ¨ä¸€ä¸ªé…ç½®æ–‡ä»¶é‡Œé¢ï¼Œç„¶åä»¥ä¸€ä¸ªä¸»å‡½æ•°ä½œä¸ºå…¥å£ç›´æ¥è¿è¡Œï¼Œæ‰€ä»¥æ‰æœ‰äº†è¿™ä¸ªé¡¹ç›®ï¼Œå–œæ¬¢æŒ‡ä»¤çš„æœ‹å‹ä»¬å¯ä»¥æ”¹å›å»ä½¿ç”¨ã€‚  


## Updates
Date| Detail
:---|---
2023-07-25|åˆå§‹ä»“åº“
2023-08-08|å¥–åŠ±æ¨¡å‹è®­ç»ƒ
2023-08-21|RLHFçš„PPOæ–¹æ³•å¯¹å„ä¸ªæ¨¡å‹çš„è®­ç»ƒæ”¯æŒ
2023-08-23|RLHFçš„DPOæ–¹æ³•å¯¹å„ä¸ªæ¨¡å‹çš„è®­ç»ƒæ”¯æŒ
2023-08-24|æ”¯æŒdeepspeed-ZeRo2åˆ†å¸ƒå¼è®­ç»ƒ
2023-09-04|æ”¯æŒéƒ¨åˆ†å¯ä»¥ä»é…ç½®ä¿®æ”¹ä½¿ç”¨NTKçš„æ¨¡å‹
2023-09-11|å¤šè½®å¯¹è¯çš„[Fireflyçš„loss](https://mp.weixin.qq.com/s/nhogoWnzl3nrs_77r38_UA)è®­ç»ƒå‡½æ•°é›†æˆ

## Requirement
å‡ ä¸ªé‡è¦ç¯å¢ƒï¼š
* pythonï¼š3.10+  
* torchï¼š2.0.1+  
* bitsandbytesï¼šä¸åŒæ“ä½œç³»ç»Ÿä¸‹éœ€è¦å¯¹åº”å®‰è£…ä¸åŒçš„åŒ…ï¼ˆLinuxä¸‹0.39.0+ï¼ŒWindowsä¸‹è¦ä¸“é—¨ä¸‹è½½å¯¹åº”çš„wheelæœ¬åœ°å®‰è£…ï¼‰

å…¶å®ƒç¯å¢ƒè§requirements.txt

## Feature

### Supported models
å¤§æ¨¡å‹ç»è¿‡SFT(ç„¶ååšRLHF)ä¹‹åå¯ç”¨äºå¯¹è¯ä»»åŠ¡Chatï¼Œé¢ä¸–çš„Chatå¤§éƒ¨åˆ†éƒ½æ²¡æœ‰é‡æ–°è®­ç»ƒåŸºåº§ï¼Œæˆ–è€…æ˜¯åŸºäºåŒæ ·çš„åŸºåº§ç»“æ„ç”¨æ•°æ®é‡æ–°é¢„è®­ç»ƒäº†ä¸€ä¸ªåŸºåº§ï¼Œä¸‹è¡¨æ˜¯éªŒè¯è¿‡çš„è¢«æ­¤é¡¹ç›®æ”¯æŒçš„åŸºåº§ï¼Œç›¸åº”çš„ä¹Ÿæ”¯æŒåŒæ ·ç»“æ„çš„è¡ç”Ÿå’ŒChatæ¨¡å‹ã€‚

Model    | Scale        | Series
:--------|--------------|--------
ChatGLM1 | 6B           |[chatglm1](https://huggingface.co/THUDM/chatglm-6b)
ChatGLM2 | 6B           |[chatglm2](https://huggingface.co/THUDM/chatglm2-6b)
Qwen     | 7B           |[Qwen](https://huggingface.co/Qwen)
Bloom    | 560Mã€9Bã€7B1M|[bloom](https://huggingface.co/bigscience/bloom)ã€[bloomz](https://huggingface.co/bigscience/bloomz)
LLama1   | 3Bã€7Bã€13B   |[openllama](https://huggingface.co/openlm-research)ã€[chinese-alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)ã€[ziya](https://huggingface.co/IDEA-CCNL)
LLama2   | 7Bã€13B      |[llama2](https://huggingface.co/meta-llama)
Baichuan | 7Bã€13B      |[baichuan](https://huggingface.co/baichuan-inc)
Baichuan2| 7Bã€13B      |[baichuan2](https://huggingface.co/baichuan-inc)
Falcon   | 7B           |[falcon](https://huggingface.co/tiiuae/falcon-7b)ã€[chinese-Falcon](https://huggingface.co/Linly-AI)
Aquila   | 7B           |[aquila](https://huggingface.co/BAAI)
InternLM | 7B           |[internlm](https://huggingface.co/internlm)
MOSS     | 16B          |[MOSS](https://huggingface.co/fnlp)
XVERSE   | 13B          |[XVERSE](https://huggingface.co/xverse/XVERSE-13B-Chat)

* ä½¿ç”¨RWKVæ—¶å€™éœ€è¦ä½¿ç”¨æœ¬é¡¹ç›®çš„[convert_rwkv_to_hf](engines/utils/convert_rwkv_to_hf.py)æˆ–è€…transformersè‡ªå¸¦çš„[convert_rwkv_checkpoint_to_hf](https://github.com/huggingface/transformers/blob/main/src/transformers/models/rwkv/convert_rwkv_checkpoint_to_hf.py)å°†æ¨¡å‹è½¬æˆhfæ ¼å¼ã€‚
* æœªè¿›å…¥ä¸‹è¡¨çš„æ¨¡å‹æˆ–å‚æ•°è§„æ¨¡æš‚æ—¶æ²¡æœ‰ä½¿ç”¨è¯¥é¡¹ç›®è·‘è¿‡ã€‚

### Training methods  

Method        |Supported| 
:-------------|---------|
Full Parameter| âœ…     |
Lora          | âœ…     |
AdaLora       | âœ…     |
QLora         | âœ…     |
Prompt Tuning | âœ…     |
P Tuning      | âœ…     |
Prefix Tuning | âœ…     |

* ä½¿ç”¨Loraå’ŒAdaLoraéƒ½æ”¯æŒQLoraè®­ç»ƒï¼Œä½†æ˜¯é‡åŒ–æ–¹å¼éœ€è¦é€‰æ‹©åŸºäºbitsandbytesçš„bnbé‡åŒ–æ–¹å¼ï¼Œå¯æ”¯æŒ4bitå’Œ8bité‡åŒ–è®­ç»ƒï¼Œå› ä¸ºPeftåœ¨0.4.0ç‰ˆæœ¬åé›†æˆäº†è¯¥é‡åŒ–æ–¹å¼çš„æ¨¡å‹åŠ è½½ï¼Œå¯ä»¥ç›´æ¥é‡åŒ–åè®­ç»ƒã€‚

### Quantization

ä¸¤ç§é‡åŒ–æ–¹å¼åˆ†åˆ«ä¸ºåŸºäºbitsandbytesçš„bnbå’Œcpm_kernelsç»„ä»¶çš„cpmï¼Œå…¶ä¸­cpmé‡åŒ–è„šæœ¬æ¥è‡ª[quantization.py](https://huggingface.co/THUDM/chatglm2-6b/blob/main/quantization.py)ã€‚

### Metric
ä¸åŒè®­ç»ƒé˜¶æ®µè·‘æµ‹è¯•é›†æ—¶ä¼šè¾“å‡ºä¸‹é¢ä¸€äº›å¸¸è§„çš„ç”Ÿæˆæ¨¡å‹è¯„ä¼°ç»“æœï¼Œç»“æœä»…é™å‚è€ƒï¼Œå¤§æ¨¡å‹çš„äº‹å®æ€§è¯„ä¼°ç›®å‰æ²¡æœ‰æ›´å¥½çš„åŠæ³•ï¼Œéƒ½æ˜¯å„ä¸ªæ¨¡å‹å‡ºå“æ–¹æˆ–è¯„æµ‹æœºæ„åœ¨å„ç»´åº¦ä¸Šåˆ¶ä½œæ•°æ®é›†åšè¯„æµ‹ï¼Œç›¸å¯¹æ¯”è¾ƒä¸»è§‚ã€‚   

Metric  |Supported| Training Stage|
:-------|---------|---------------|
Rouge-1 | âœ…     |SFT Training    |
Rouge-2 | âœ…     |SFT Training    |
Rouge-l | âœ…     |SFT Training    |
ppl     | âœ…     |SFT Training    |
accuracy| âœ…     |PPO-RM Training |

## Getting start
å¼€å§‹ä¹‹å‰ï¼Œéœ€è¦ç¡®å®šè¯•éªŒçš„æ¨¡å‹ï¼Œå¹¶æŠŠæ•´ä¸ªæ¨¡å‹æ–‡ä»¶ä»huggingfaceä¸Šä¸‹è½½ä¸‹æ¥ï¼Œå®Œæˆä¸¤æ­¥ï¼š
1. åœ¨ModelArgumentsä¸­é…ç½®å¥½model_typeå’Œmodel_pathä¸¤ä¸ªå‚æ•°ï¼Œå¦‚æœé™¤äº†model_pathçš„åŸºåº§æ¨¡å‹å¤–è¿˜æœ‰adapteræ¨¡å‹ï¼Œåˆ™éœ€å°†adapteræ¨¡å‹çš„åœ°å€é…ç½®åˆ°checkpoint_dirä¸­ã€‚

```
model_type: str = field(
    default='internlm',
    metadata={
        # æ¨¡å‹ç±»å‹
        'help': 'Model type.',
        'choices': ['chatglm', 'qwen', 'llama', 'falcon', 'baichuan', 'aquila', 'internlm', 'moss', 'bloom', 'rwkv'],
    }
)
model_path: str = field(
    default='/home/XXXXX/llm_models/internLM/intern-chat-7b',
    metadata={
        # ä»huggingface.co/modelsä¸Šä¸‹è½½çš„æ¨¡å‹ä¿å­˜åˆ°æœ¬åœ°çš„è·¯å¾„ã€‚
        'help': 'Local path to pretrained model or model identifier from huggingface.co/models.'
    }
)
checkpoint_dir: Optional[str] = field(
    default=None,
    metadata={
        # ä¿å­˜ä¸‹è½½çš„æˆ–è€…è‡ªå·±è®­ç»ƒçš„adapterå¢é‡æ¨¡å‹çš„åœ°æ–¹ã€‚
        'help': 'Path to save the (delta) model checkpoints as well as the configurations automatically.',
    }
)
```
2. åœ¨DataTrainingArgumentsä¸­ä¿®æ”¹prompt_templateä½¿ç”¨å’Œè¯¥æ¨¡å‹é…å¥—çš„templateï¼Œè¿™ä¸ªtemplateä¸€èˆ¬æ˜¯SFTä¹‹åçš„æ¨¡å‹æ‰ä¼šæœ‰ï¼Œä¸”ä¸è®­ç»ƒè€…æœ‰å…³ã€‚æ‰€ä»¥å¦‚æœè¯¥é¡¹ç›®æœªæä¾›çš„ï¼Œåˆ™éœ€è¦è‡ªå·±ä¿®æ”¹engines/utils/prompt_template.pyæ–‡ä»¶ï¼Œæ·»åŠ æ–°çš„templateã€‚
```
prompt_template: Optional[str] = field(
    default='internlm',
    metadata={
        # é€‰æ‹©å¯¹åº”æ¨¡å‹çš„æ¨¡æ¿promptï¼Œä¸€èˆ¬Chatæ¨¡å‹çš„å‡ºå“æ–¹éƒ½ä¼šæœ‰ä¸€ä¸ªå›ºå®šçš„promptã€‚
        'help': 'Which template to use for constructing prompts in training and inference.'
    }
)
```

### Inference  
æ­¤å¤„æä¾›ä¸¤ç§é¢„æµ‹æ–¹å¼ï¼Œåˆ†åˆ«æ˜¯åŸºäºgradioçš„webUIé¢„æµ‹å’Œç»ˆç«¯é¢„æµ‹ã€‚éœ€è¦åœ¨config.pyä¸­å¯¹åº”ä¿®æ”¹modeï¼Œç„¶åè¿è¡Œmain.pyã€‚  

Mode              | Inference Type | 
:-----------------|----------------|
web_inference     | WebUI          |
terminal_inference| Trminal        |

* é¢„æµ‹çš„æ—¶å€™ï¼Œæ¨¡å‹ä¼šä¼˜å…ˆä»ä½ å®šä¹‰çš„ModelArgumentsä¸­çš„checkpoint_dirè¯»å–ï¼Œå¦‚æœè¯¥æ–‡ä»¶ä¸‹æ²¡æœ‰å‚æ•°æ–‡ä»¶ï¼Œåˆ™ä»TrainingArgumentsçš„output_diræ–‡ä»¶å¤¹åŠ è½½ï¼Œå¦‚æœéƒ½æ²¡æœ‰åˆ™åªåŠ è½½æœ€åˆçš„åŸºåº§æ¨¡å‹ã€‚

#### NTK
ç›®å‰åŸç”Ÿçš„configå°±èƒ½æ”¯æŒNTKæ–¹æ³•çš„æœ‰[chatglm2-6b-32k](https://huggingface.co/THUDM/chatglm2-6b-32k)ã€LLamaç³»åˆ—ã€Falconç³»åˆ—å’Œ[Qwen-7B-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat)ï¼š
Model          |Position Encoder|Support NTK Type| 
:--------------|----------------|----------------|
chatglm2-6b-32k| Rope           |  Linear        |
Qwen-7B-Chat   | Rope           | Dynamic        |
LLamaç³»åˆ—      | Rope            |Dynamicã€Linear |
Falconç³»åˆ—     | Rope           |Dynamicã€Linear |

* å…¶ä»–çš„æ¨¡å‹éœ€è¦è‡ªå·±æ›´æ”¹åŸå§‹çš„æ¨¡å‹æ–‡ä»¶å»æ”¯æŒNTKæ–¹æ³•ï¼Œæ¯”å¦‚å¯ç”¨äºAlibiç¼–ç çš„æ¨¡å‹Baichuanã€Falconã€Bloomç³»åˆ—çš„[NTK-ALibi](https://github.com/keezen/ntk_alibi)ã€‚ä¸€èˆ¬æ¥è¯´ï¼ŒNTKä¸»è¦ç”¨åœ¨æ¨æ–­çš„æ—¶å€™çªç ´æ¨¡å‹çš„è¾“å…¥tokené™åˆ¶ï¼Œä½†æ˜¯è®­ç»ƒçš„æ—¶å€™æ‰“å¼€NTKå¯èƒ½ä¼šå¾—ä¸åˆ°æƒ³è¦çš„æ•ˆæœã€‚
* Falconç³»åˆ—çš„æ¨¡å‹HFå®˜æ–¹æä¾›äº†ä¸¤ç§ç¼–ç æ–¹å¼ï¼Œåˆ†åˆ«æ˜¯Ropeå’ŒAlibiï¼Œä½†æ˜¯tiiuaeå®˜æ–¹ç›®å‰åªæœ‰Alibiçš„å®ç°ï¼Œä¸çŸ¥é“æ­¤ä¸¾ä¸ºä½•ï¼Œæ‰€ä»¥æ­¤å¤„ä»…æ”¯æŒä½¿ç”¨Ropeç¼–ç æ–¹å¼çš„NTKæ–¹æ³•ã€‚

### SFT training

#### è®­ç»ƒæ•°æ®
æŒ‡ä»¤å¾®è°ƒæ•°æ®å‚è€ƒdatasets/finetune/example/trainä¸‹é¢çš„æ–‡ä»¶ï¼Œæ•°æ®ç”±instructionã€inputã€outputå’Œhistoryå››ä¸ªå­—æ®µç»„æˆã€‚
```
[
  {
    "instruction": "10ä¹˜ä»¥10ç­‰äºå¤šå°‘ï¼Ÿ",
    "input": "",
    "output": "10ä¹˜ä»¥10ç­‰äº100ã€‚",
    "history": [
        "ä½ å¥½å‘€ã€‚",
        "ä½ å¥½ï¼Œè¯·é—®æ‚¨æœ‰ä»€ä¹ˆéœ€è¦å¸®åŠ©çš„å—ï¼Ÿ",
        "å¥½çš„ï¼Œæˆ‘æƒ³é—®ä¸‹ä½ æ˜¯è°ï¼Ÿ",
        "æˆ‘æ˜¯ä¸€ä¸ªAIæ¨¡å‹ï¼Œèƒ½å¤Ÿè§£å†³ä½ æå‡ºçš„é—®é¢˜ã€‚"
      ]
  },
  ...  
]
```
å¦‚ä¸Šé¢æ‰€ç¤ºhistoryå­—æ®µéœ€è¦æŒ‰ç…§ä¸€é—®ä¸€ç­”çš„æ ¼å¼å­˜å‚¨å¯¹è¯å†å²ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒã€‚å¦‚æœæ²¡æœ‰å†å²å¯¹è¯éœ€è¦è®©historyä¸ºç©ºåˆ—è¡¨ï¼š
```
[
  {
    "instruction": "ä½ èº«ä»½æ˜¯ä»€ä¹ˆï¼Ÿ",
    "input": "",
    "output": "æˆ‘æ˜¯ä¸€ä¸ªAIæ™ºèƒ½åŠ©æ‰‹ï¼Œç”±XXå…¬å¸è®­ç»ƒï¼Œæˆ‘å°†åŠ›æ‰€èƒ½åŠçš„å¸®åŠ©ä½ è§£å†³é—®é¢˜ã€‚",
    "history": []
  },
  ...  
]
```

ä½¿ç”¨çš„æ—¶å€™æŠŠæ•°æ®è·¯å¾„å¡«å†™åˆ°DataTrainingArgumentsé…ç½®é‡Œé¢ï¼š
```
train_file_dir: Optional[str] = field(
    default='datasets/finetune/train',
    metadata={
        # è®­ç»ƒé›†ä¿å­˜çš„è·¯å¾„ã€‚
        'help': 'The train json data file folder.'
    }
)
validation_file_dir: Optional[str] = field(
    default='datasets/finetune/test',
    metadata={
        # éªŒè¯é›†ä¿å­˜çš„è·¯å¾„ã€‚
        'help': 'The evaluation json file folder.'
    }
)
```

#### è®­ç»ƒé…ç½®
éœ€è¦åœ¨config.pyä¸­å¯¹åº”ä¿®æ”¹modeä¸ºsft_trainï¼Œç„¶ååœ¨TrainingArgumentsä¸­é…ç½®å¥½å„é¡¹è®­ç»ƒå‚æ•°ï¼Œç„¶åè¿è¡Œmain.pyã€‚å¸¸ç”¨çš„ä¸€äº›å‚æ•°å¦‚ä¸‹ï¼š

Arguments                    | Describe                | 
:----------------------------|-------------------------|
fine_tuning_type             | è®­ç»ƒæ–¹å¼                  |
use_firefly_loss             | ä½¿ç”¨Firefly lossè®­ç»ƒæ¨¡å‹   |
output_dir                   | è®­ç»ƒç»“æœè¾“å‡ºçš„æ–‡ä»¶å¤¹        |
num_train_epochs             | è®­ç»ƒçš„è½®æ¬¡                 |
gradient_accumulation_steps  | æ¢¯åº¦ç´¯ç§¯                   |
per_device_train_batch_size  | æ¯ä¸ªè®¾å¤‡ä¸Šçš„æ‰¹å¤§å°           |
learning_rate                | å­¦ä¹ ç‡                    |
fp16                         | è®¾ç½®Trueä¸ºå¼€æ··åˆç²¾åº¦è¿ç®—     |


* éœ€è¦ä½¿ç”¨deepspeedçš„æ—¶å€™ï¼Œå°†é…ç½®æ–‡ä»¶çš„jsonè·¯å¾„ï¼Œå¡«å†™åˆ°TrainingArgumentsçš„deepspeedå‚æ•°ä¸­ã€‚
* Loraå’Œå…¶å®ƒadapterè®­ç»ƒæ–¹å¼çš„é…ç½®å‚æ•°ä¹Ÿåœ¨TrainingArgumentsä¸­ï¼Œè¿™é‡Œé¢è¦æ³¨æ„lora_targetçš„è®¾ç½®è¦æ ¹æ®è‡ªå·±çš„æ¨¡å‹ç»“æ„æ¥ï¼Œé…ç½®ä¸­ç»™äº†ä¸€äº›å‚è€ƒã€‚
* QLoraåªæ”¯æŒLoraå’ŒAdaLoraä¸¤ç§æ–¹å¼ï¼Œé‡åŒ–æ–¹å¼éœ€è¦é€‰æ‹©bnbï¼Œæ”¯æŒint4å’Œint8ä¸¤ç§é‡åŒ–ã€‚
* Firefly Lossä»…ä½œç”¨åœ¨SFTè®­ç»ƒé˜¶æ®µä¸”ä¸æ”¯æŒChatGLM6Bç­‰Prefix LMæ¨¡å‹ã€‚

```
quantization: Optional[str] = field(
    default='bnb',
    metadata={
        # å¦‚æœä½¿ç”¨qloraåªèƒ½é€‰æ‹©bnbï¼Œä¸¤ç§é‡åŒ–æ–¹å¼åŒºåˆ«ä¸å¤§ã€‚
        'help': 'The specific model version to use (can be a branch name, tag name or commit id).',
        'choices': ['cpm', 'bnb'],
    }
)
quantization_bit: Optional[int] = field(
    default=None,
    metadata={
        # ä½¿ç”¨8bité‡åŒ–è¿˜æ˜¯4bité‡åŒ–ï¼Ÿ
        'help': 'The number of bits to quantize the model.',
        'choices': [4, 8],
    }
)
```

### RM training
#### è®­ç»ƒæ•°æ®
æŒ‡ä»¤å¾®è°ƒæ•°æ®å‚è€ƒdatasets/rm/example/trainä¸‹é¢çš„æ–‡ä»¶ï¼Œæ•°æ®ç”±instructionã€inputã€outputä¸‰ä¸ªå­—æ®µç»„æˆã€‚outputæ˜¯ä¸€ä¸ªä¸¤å…ƒç´ åˆ—è¡¨ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯é‡‡çº³çš„ç­”æ¡ˆï¼Œç¬¬äºŒä¸ªæ˜¯æ‹’ç»çš„ç­”æ¡ˆã€‚  
ä½¿ç”¨çš„æ—¶å€™æŠŠè®­ç»ƒå¥–åŠ±æ¨¡å‹çš„æ•°æ®SFTé‡Œé¢ä¸€æ ·å¡«å†™åˆ°DataTrainingArgumentsé…ç½®é‡Œé¢ã€‚

#### è®­ç»ƒé…ç½®
éœ€è¦åœ¨config.pyä¸­å¯¹åº”ä¿®æ”¹modeä¸ºrm_trainï¼Œç„¶ååœ¨TrainingArgumentsä¸­é…ç½®å¥½å„é¡¹è®­ç»ƒå‚æ•°ï¼Œç„¶åè¿è¡Œmain.pyã€‚å¸¸ç”¨çš„å‚æ•°å’ŒSFTä¸€æ ·ï¼Œå‚åŠ ä¸Šé¢çš„SFTè®­ç»ƒé…ç½®å†…å®¹ã€‚

* å¥–åŠ±æ¨¡å‹è®­ç»ƒä¸æ”¯æŒç¬¬ä¸€ä»£ChatGLM6Bï¼Œå› ä¸ºé¡¹ç›®ç”¨trlçš„AutoModelForCausalLMWithValueHeadç»„ä»¶æ˜¯åŸºäºCausalLMæ¨¡å‹çš„ã€‚ChatGLM6Bæ˜¯åŸºäºPrefix LMå®ç°çš„ã€‚

### Test
ä¿®æ”¹DataTrainingArgumentsä¸­çš„test_fileä¸ºæµ‹è¯•æ•°æ®é›†æ‰€åœ¨çš„è·¯å¾„ã€‚  
```
test_file: Optional[str] = field(
    default='datasets/finetune/test',
    metadata={
        # æµ‹è¯•é›†ä¿å­˜çš„è·¯å¾„ã€‚
        'help': 'The test file.'
    }
)
```

å¦‚æœè·‘æŒ‡ä»¤å¾®è°ƒçš„æµ‹è¯•ï¼Œéœ€è¦åœ¨config.pyä¸­å°†modeä¿®æ”¹ä¸ºsft_batch_testï¼Œç„¶åè¿è¡Œmain.pyã€‚  
å¦‚æœè·‘å¥–åŠ±æ¨¡å‹çš„æ‰¹é‡æµ‹è¯•ï¼Œéœ€è¦åœ¨config.pyä¸­å°†modeä¿®æ”¹ä¸ºrm_batch_testï¼Œç„¶åè¿è¡Œmain.pyï¼Œå¥–åŠ±æ¨¡å‹æµ‹è¯•åªä¼šè¾“å‡ºæ¨¡å‹çš„å‡†ç¡®ç‡ã€‚

### RLHF training
#### PPO
åœ¨è¿›è¡ŒåŸºäºPPOæ¨¡å‹çš„RLHFè®­ç»ƒä¹‹å‰ï¼Œéœ€è¦ä¸€ä¸ªå¥–åŠ±æ¨¡å‹å’Œä¸€ä¸ªéœ€è¦è¢«RLHFå¾®è°ƒçš„SFTæ¨¡å‹ï¼Œéœ€è¦æŠŠä»–ä»¬é…ç½®åˆ°ModelArgumentsä¸­å¦‚ä¸‹ï¼š
```
checkpoint_dir: Optional[str] = field(
    default='checkpoint/sft',
    metadata={
        # ä¿å­˜ä¸‹è½½çš„æˆ–è€…è‡ªå·±è®­ç»ƒçš„adapterå¢é‡æ¨¡å‹çš„åœ°æ–¹ï¼Œåœ¨RLHFæ—¶å€™ï¼Œæ­¤å¤„éœ€è¦å¡«å†™æŒ‡ä»¤å¾®è°ƒåæ¨¡å‹æ‰€åœ¨çš„æ–‡ä»¶åœ°å€ã€‚
        'help': 'Path to save the (delta) model checkpoints as well as the configurations automatically.',
    }
)
reward_model_checkpoint: str = field(
    default='checkpoint/rm',
    metadata={
        # åœ¨RLHFæ—¶å€™ï¼Œæ­¤å¤„éœ€è¦å¡«å†™å¥–åŠ±æ¨¡å‹æ‰€åœ¨çš„æ–‡ä»¶åœ°å€
        'help': 'The checkpoint of reward model.'
    }
)
```  
PPOæ–¹æ³•å¯¹æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ•°æ®å’ŒSFTçš„æ•°æ®æ˜¯ä¸€è‡´çš„ï¼Œæ­¤å¤–ä½¿ç”¨çš„æ—¶å€™è¿˜éœ€è¦åœ¨TrainingArgumentsä¸­æŠŠPPOçš„é…ç½®å¡«å†™å¥½ï¼Œåœ¨config.pyä¸­å°†modeä¿®æ”¹ä¸ºppo_trainï¼Œç„¶åè¿è¡Œmain.pyã€‚è®­ç»ƒçš„ç»“æœå°†ä¼šé€šè¿‡wandbçš„æ ¼å¼è®°å½•åœ¨è®­ç»ƒè¾“å‡ºçš„æ–‡ä»¶å¤¹ä¸­ã€‚

#### DPO
åœ¨è¿›è¡ŒåŸºäºDPOæ¨¡å‹çš„RLHFè®­ç»ƒä¹‹å‰ï¼Œåªéœ€è¦ä¸€ä¸ªè¢«RLHFå¾®è°ƒçš„SFTæ¨¡å‹ï¼Œå¦‚æœæ˜¯åŸºäºadapterçš„æ¨¡å‹è¿˜éœ€è¦æŠŠadapteré…ç½®åˆ°ModelArgumentsä¸­å¦‚ä¸‹ï¼š
```
model_path: str = field(
    default='/home/XXX/ChatGLM/ChatGLM2-6B-32k',
    metadata={
        # ä»huggingface.co/modelsä¸Šä¸‹è½½çš„æ¨¡å‹ä¿å­˜åˆ°æœ¬åœ°çš„è·¯å¾„æˆ–è€…è‡ªå·±çš„æ¨¡å‹ã€‚
        'help': 'Local path to pretrained model or model identifier from huggingface.co/models.'
    }
)
checkpoint_dir: Optional[str] = field(
    default='checkpoint/sft',
    metadata={
        # ä¿å­˜ä¸‹è½½çš„æˆ–è€…è‡ªå·±è®­ç»ƒçš„adapterå¢é‡æ¨¡å‹çš„åœ°æ–¹ï¼Œåœ¨RLHFæ—¶å€™ï¼Œæ­¤å¤„éœ€è¦å¡«å†™æŒ‡ä»¤å¾®è°ƒåæ¨¡å‹æ‰€åœ¨çš„æ–‡ä»¶åœ°å€ã€‚
        'help': 'Path to save the (delta) model checkpoints as well as the configurations automatically.',
    }
)
```  
DPOæ–¹æ³•å¯¹æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ•°æ®å’Œå¥–åŠ±æ¨¡å‹çš„æ•°æ®æ˜¯ä¸€è‡´çš„ï¼Œåœ¨config.pyä¸­å°†modeä¿®æ”¹ä¸ºdpo_trainï¼Œç„¶åè¿è¡Œmain.pyã€‚è®­ç»ƒçš„ç»“æœå°†ä¼šé€šè¿‡wandbçš„æ ¼å¼è®°å½•åœ¨è®­ç»ƒè¾“å‡ºçš„æ–‡ä»¶å¤¹ä¸­ã€‚


* å¦‚æœå‰é¢ä½¿ç”¨çš„æ˜¯adapteråœ¨SFTæ¨¡å‹ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼ŒRLHFçš„æ—¶å€™é¡¹ç›®ä¼šèåˆå‰é¢çš„adapterååˆ›å»ºæ–°çš„adapterç»§ç»­è®­ç»ƒã€‚

### DeepSpeed
ä½¿ç”¨deepspeedè¿›è¡Œè®­ç»ƒéœ€è¦åœ¨TrainingArgumentsæŒ‡å®šdeepspeedçš„configæ–‡ä»¶(é¡¹ç›®ä¸­æä¾›äº†stage2çš„deepspeedé…ç½®)ï¼š
```
deepspeed: Optional[str] = field(
    default='deepspeed_configs/zero_stage2_config.json',
    metadata={
        'help': 'Enable deepspeed and pass the path to deepspeed json config file (e.g. ds_config.json) '
                'or an already loaded json file as a dict'
    }
)
```
é…ç½®å¥½ååœ¨ç»ˆç«¯è¾“å…¥(å•æœºå¤šå¡)ï¼š
```
deepspeed --num_gpus 3 --master_port=9901 main.py
```

* å¤šæœºå¤šå¡éœ€è¦æŒ‡å®šæ›´å¤šçš„å‚æ•°ï¼Œå¯ä»¥å‚è€ƒhugingfaceçš„deepspeedæ–‡æ¡£ã€‚

### Others
Mode                | Describe                                                                                                      | 
:-------------------|---------------------------------------------------------------------------------------------------------------|
merge_lora_model    | å°†loraæ¨¡å‹å’ŒåŸºåº§æ¨¡å‹èåˆï¼Œæ”¯æŒloraå’Œadaloraä¹‹åçš„æƒé‡åˆå¹¶ï¼Œå…¶å®ƒçš„è®­ç»ƒæ–¹æ³•äº§ç”Ÿçš„adapterç›´æ¥é€šè¿‡peftåŠ è½½å³å¯ï¼Œä¸æ”¯æŒåˆå¹¶|
show_model_info     | æ‰“å°æ¨¡å‹çš„ç»“æ„å’Œæ¨¡å‹çš„å‚æ•°                                                                                      |
save_quantized_model| é‡åŒ–å¹¶ä¿å­˜é‡åŒ–æ¨¡å‹                                                                                             |

* merge_peft_modelå’Œsave_quantized_modeléœ€è¦åœ¨ModelArgumentsè®¾ç½®è¾“å‡ºåœ°å€ã€‚
```
quantized_or_merged_output_dir: Optional[str] = field(
    default=None,
    metadata={
        # å½“ä½ æƒ³ä¿å­˜é‡åŒ–åçš„æ¨¡å‹æˆ–è€…èåˆåçš„æ¨¡å‹æ—¶ï¼Œå¤„ç†åçš„æ¨¡å‹ä¿å­˜çš„åœ°å€ã€‚
        'help': 'Path to save the quantized or merged model checkpoints as well as the configurations manually.',
    }
)
```
* ä½¿ç”¨bnbå’Œcpmé‡åŒ–å°†ä¼šé»˜è®¤å¯¹é™¤äº†è¾“å‡ºå±‚çš„æ‰€æœ‰çº¿æ€§å±‚è¿›è¡Œé‡åŒ–ã€‚

```
cpm_quantization_target: Optional[str] = field(
    default='query_key_value',
    metadata={
        # éœ€è¦å¯¹è¿™ä¸ªæ¨¡å‹é‡Œé¢çš„å“ªäº›çº¿æ€§å±‚è¿›è¡Œé‡åŒ–ï¼Ÿ
        'help': "Name(s) of target modules to use cpm Quantize. Use comma to separate multiple modules.
    }
)
```

## Todo
- [x] å¥–åŠ±æ¨¡å‹è®­ç»ƒ
- [x] PPOæ¨¡å‹è®­ç»ƒ
- [x] DPOæ¨¡å‹è®­ç»ƒ
- [x] æ”¯æŒDeepspeedè®­ç»ƒ
- [x] [NTK-Aware Scaled RoPE](https://kexue.fm/archives/9706)é›†æˆ
- [x] å¤šè½®å¯¹è¯çš„[Fireflyçš„loss](https://mp.weixin.qq.com/s/nhogoWnzl3nrs_77r38_UA)å‡½æ•°é›†æˆ
- [ ] mmluã€cmmluå’ŒC-Evalè‡ªåŠ¨åŒ–è¯„ä¼°


## Citation

å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†è¯¥é¡¹ç›®ï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š

```latex
@misc{LLMs Tool,
  title={LLMs Tool: a tool for large language models},
  author={Shouxian Li},
  year={2023},
  howpublished={\url{https://github.com/StanleyLsx/llms_tool}},
}
```
